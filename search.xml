<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[动态定价-解读Airbnb使用的模型]]></title>
    <url>%2F2018%2F11%2F%E5%8A%A8%E6%80%81%E5%AE%9A%E4%BB%B7-%E8%A7%A3%E8%AF%BBAirbnb%E4%BD%BF%E7%94%A8%E7%9A%84%E6%A8%A1%E5%9E%8B.html</url>
    <content type="text"><![CDATA[随着饭点儿业务流量不断的提升，预购产品在未来可期范围必将深入行业，技术的支撑应该提前做出响应，在此和大家一起探讨一下基于动态定价思路的折扣推荐方案。 饭点儿-最新业务发展基本情况 饭点儿-预购产品示意图 什么是动态定价？动态定价(Dynamic pricing)是指企业根据市场需求和自身供应能力，以不同的价格将同一产品适时地销售给不同的消费者或不同的细分市场，以实现收益最大化的策略。 基于时机定价策略：关键在于把握顾客不同时间对价格承受的心理差异。高峰负荷定价（peak-load pricing）：供不应求，供应缺乏弹性，应合理提价清理定价（clearance pricing）：需求状况不确定、容易贬值，须降低价格，及时清理多余库存。 基于市场细分与限量配给策略：根据不同的产品配置、渠道、客户类型和时间等，进行区别定价，如：航空公司。 基于动态推销策略:利用互联网赋予的强大优势，根据供应和库存水平的变化，迅速、频繁地实施价格调整，为顾客提供不同的产品、各种促销优惠、多种交货方式以及差异化的产品定价。例如，在亚马逊购书，书店会根据消费记录，给与个性化的购书建议。 基于三种动态定价的综合运用在实际运用过程中，可酌情考虑单独实施某一策略，或进行策略组合。顾客价格承受心理差异性越强，市场需求的不确定因素越多，这些策略的价值及其作用也就越大。 好文解读-Customized Regression Model for Airbnb Dynamic Pricing 文章来源：Peng Ye, Julian Qian, Jieying Chen, Chen-hung Wu, Yitong Zhou, Spencer De Mars, Frank Yang, and Li Zhang. 2018. Customized Regression Model for Airbnb Dynamic Pricing. In KDD ’18: The 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, August 19–23, 2018, London, United Kingdom. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3219819.3219830Airbnb是一个用于共享住宿的在线平台，在这里寻求住宿的客人和有空余房间共享的主人可以完成在线匹配。 本文详细介绍了Airbnb用于向房主提供建议价格的方法（房主最终仍然控制着Airbnb平台的定价）。提出的模型已在Airbnb部署生产超过1年。模型的第一次迭代的推出为预订价值带来了显著收益。 本文提出的方法在理想情况下，为了使得利润最大化，我们一般会估计一个需求函数F(P), 即在给定价格P的情况下估计房间的需求量，之后选择P的值，使P×F(P)的值最大。但针对Airbnb需求函数不仅仅和价格有关，应该是F(P, t, id)，其中t是时间，id代表独立的房间。时间上因素上，需求受季节性和特殊事件的影响，以及预定早晚的影响。另外，Airbnb上房子各有不同，房子评分、浏览量等一些因素也导致需求函数的变化。而且，由于Airbnb并不是直接控制房间价格的，而是只能给出“建议价格”，所以想要直接拿不同的定价策略做实验来观察市场反应也是不容易的。 Airbnb最终确定的定价系统有三个组成部分： 首先构建一个基于Gradient Boosting Machine (GBM)的预定概率模型，用于预测某一晚某房间被预定的概率。 然后这些预测结果会被作为特征输入到定价策略模型，对空闲日期提出建议价格。 最终策略模型会加入其他个性化逻辑，以符合房东的目标、结合特殊事件等等。 预定概率模型想知道某间房会有多大概率被预定，这里是用的是Gradient Boosting Machines(GBM)模型，是一个二分类的问题。除了对全局样本通过GBM训练建模以外，也给每个市场训练单独的模型，训练数据的抽样率根据市场密度而变化，这些模型考虑了三种不同类型的特征： 房屋属性特征：每晚的房价，房间类型，人数，卧室/浴室数量，设施，位置，评论，历史入住率，是否启用即时预订等 时间特征：季节因素、可入住时长、预定早晚等。 供给特征：周围房屋情况、浏览量、搜索率等。 通过在一定范围内的不同价格点对预订概率模型进行评分，可以获得估计的需求曲线 但是，由于上述的特征影响因素太多，想得到精确的需求曲线进行价格设置是非常困难的。主要的影响分三类： 数据稀疏性：大部分房屋都不怎么会经常调整价格，而且变化基本都在一个小范围，因此难以获得比较偏的观察样本。 样本唯一性：由于房屋的独特性，难以通过学习提高模型泛化性。 特征共线性：不可避免，很多特征都和价格有关，比如搜索率、浏览量、预定占比等等 尝试了直接应用利润最大化策略，但是A/B测试结果显示这些方法在实际中通常会失败。所以，我们决定找其他的替代方法。替代法使用预定概率模型的输出结果作为定价策略模型的输入因素之一。 定价策略模型首先让我们思考：在缺少最优价的基础上，训练定价策略模型时，应该用什么评估尺度呢？假设P为实际房间一晚的价格，Psug为建议价格，最优价格为P0（如果存在），我们认为以下情况Psug是不好的： 当房间被预定了且Psug &lt; P 当房间没有被预定且Psug &gt;= P 还有两种情况不能确定建议是好是坏： 当房间被预定了且Psug &gt;= P 当房间没有被预定且Psug &lt; P Item Booking Non-Booking Psug &gt;= P a b Psug &lt; P c d 定义以下几种度量： Price Decrease Recall (PDR)：所有没有被预定的样本中，建议价格低于实际价格的比例PDR = \frac{d}{b + d}如上图例子，PDR = 0.6 Price Decrease Precision (PDP)：所有Psug &lt; P的样本中，没有被预定的房间比例PDP = \frac{d}{c + d} Price Increase Recall (PIR)：所有被预定的样本中，建议价格高于或等于实际价格的比例PIR = \frac{a}{a + c} Price Increase Precision (PIP)：所有Psug &gt;= P的样本中，被预定的房间比例PIP = \frac{a}{a + b} Booking Regret (BR)：预定反悔率，所有被预定的样本中，取零和预定价格与建议价格之间差的百分比之中的最大值，然后从这些值中取中位数BR = median_{bookings}(max(0, \frac{P - Psug}{P}))如上例子，BR = Median(14%,5%, 6%, 0, 0) = 5%。本文采用PDR和BR两个度量作为评价指标。 目标函数给定N个训练样本${xi, yi}_{i=1}^N$，其中，$xi$代表关于房间特征的输入，$yi$代表预定状态，1为预定，0为未预定。特征$xi$包含以下内容： 房主设定的价格$P_i$。 上面预定概率模型得出的房间被预定的概率$q_i$ 市场需求信号：这有利于模型对市场动向做出及时反应。 对于$xi$来说，建议价格定义为$f_\theta(xi)$，$\theta$为需要学习的参数集合，自定义的损失函数受启发于SVR中$\epsilon$-不敏感损失函数，虽然我们不知道真实的最优价格，但是可以根据实际情况定义一个合理价格的区间，认为最优价格应该落在其中。 损失函数如下： L = arg min_\theta \sum_{i=1}^N (L(P_i,y_i) - f_\theta(x_i))^+ + (f_\theta(x_i) - U(P_i,y_i))^+其中，“+” 表示 $max(0, \cdot)$，$L(P_i,y_i)$ 、$U(P_i,y_i)$分别表示最优价格区间的上界和下界，如果建议价格落在上界和下界之间，那么损失=0，否则，损失等于建议价格与界限的距离，上界和下界分别定义为： L(P_i,y_i) = y_i \cdot P_i + (1 - y_i) \cdot c_1P_iU(P_i,y_i) = (1 - y_i) \cdot P_i + y_i \cdot c_2P_i其中，$c_1 \in(0,1)$，$c_2 &gt; 1$，下图分别说明了损失函数在正负样本情况下的变化方向。 对于正样本来说，当建议价格落在区间$(P_i, c_2P_i)$时，损失函数会降为0，与此同时预定反悔率BR也达到最小0。同样，对于负样本来说，当建议价格落在区间$(c_1P_i, P_i)$时，损失函数会降为0，与此同时PDR会很大因为建议价格比实际价格要低，这也正是模型想发展的方向。 需求增强定价法本节着重给出建议价格的具体形式，模型的背后需要满足几个前提条件的： 对于同一个房间，建议价格和被预定概率是正相关的，这样建议价格才能对概率的变化做出调整。 建议价格往往是集中在房主喜欢设定的最有代表性的价格之间左右徘徊，通过学习增加/降低的力度去控制。 预定概率模型捕捉不到的额外需求信号，应该加以关注。 基于上述假设，引入一个非对称指数模型，通过样本数据学习对实际价格的增加/减少度，建议价格Psug表示如下： Psug = P \cdot V其中，增加/减少度V定义如下： V = \begin{cases} 1 + \theta_1(q^{\varphi_H^{-qD}} - \theta_2) & \text{if $D$ > 0} \\ 1 + \theta_1(q^{\varphi_L^{-(1-q)D}} - \theta_2) & \text{if $D$]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Dynamic Pricing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改进神经网络的学习方法(1)]]></title>
    <url>%2F2018%2F06%2F%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-1.html</url>
    <content type="text"><![CDATA[本资料摘自于《神经网络与深度学习》一文，总结了一下其中提到的改进学习的方法，方法仅仅覆盖了大量已经在神经网络中研究发展出的技术的一点点内容。掌握了这些关键技术不仅仅对这些技术本⾝的理解很有用，而且会深化对使用神经网络时会遇到哪些问题的理解。 交叉熵代价函数众所周知，神经元是通过改变权重和偏置，并以⼀个代价函数的偏导数（$\partial C/\partial w$ 和 $\partial C/\partial b$）决定的速度学习。所以，我们在说“学习慢”时，实际上就是说这些偏导数很小。理解他们为何这么小就是我们面临的挑战。为了理解这些，让我们计算偏导数看看。二次代价函数如下： C(w,b) = \frac{1}{2n}\sum_{x}||y(x) - a||^2假设有个小例⼦，包含一个只有一个输入的神经元，让输入1转化为0。则对应的二次代价函数为： C = \frac{(y-a)^2}{2}其中a是神经元的输出，训练输入为$x = 1$，$y = 0$则是目标输出。显式地使用权重和偏置来表达这个，我们有$a = \sigma(z)$，其中$z = wx + b$。使用链式法则来求权重和偏置的偏导数就有： \frac{\partial C}{\partial w} = (a-y) \sigma \prime(z)x = a\sigma \prime(z)\frac{\partial C}{\partial b} = (a-y) \sigma \prime(z) = a\sigma \prime(z)当神经网络的激活函数是sigmoid函数时，函数曲线在取值为1的附近变得相当平，所以$ \sigma \prime(z)$就很小了。因此上式的偏导数会非常小。这其实就是学习缓慢的原因所在。这种学习速度下降的原因实际上也是更加一般的神经网络学习缓慢的原因，并不仅仅是在这个特例中特有的。 那么我们如何解决这个问题呢？ 假设，我们现在要训练一个包含若干输入变量的的神经元，目标输出y都是0或1。神经元的输出$a = \sigma(z)$其中： z =\sum_{j}w_jx_j + b是输入的带权和，如下定义这个神经元的交叉熵代价函数： C = -\frac{1}{n}\sum_{x}[y \ln a + (1-y)\ln(1-a)]其中n是训练数据的总数，求和是在所有的训练输入x上进行的，y是对应的目标输出。 将交叉熵看做是代价函数有两点原因： 它是非负的，C &gt; 0。 若对于所有的训练输入x，神经元实际的输出接近目标值，那么交叉熵将接近0。 但是交叉熵代价函数有一个比二次代价函数更好的特性就是它避免了学习速度下降的问题。为了弄清楚这个情况，我们来算算交叉熵函数关于权重的偏导数，得到： \frac{\partial C}{\partial w_j} = \frac{1}{n} \sum_{x} \frac{\sigma \prime(z)x_j}{\sigma(z)(1-\sigma(z))} (\sigma(z) - y)根据$\sigma(z) = 1/(1+e^{-z})$的定义，可以得到$\sigma \prime(z) = \sigma(z)(1-\sigma(z))$。代入上式可得： \frac{\partial C}{\partial w_j} = \frac{1}{n} \sum_{x} x_j(\sigma(z) - y)这是一个优美的公式。它告诉我们权重学习的速度受到$\sigma(z) - y$影响，也就是输出中的误差的控制。更大的误差，更快的学习速度，这是我们直觉上期待的效果。特别地，这个代价函数避免了像在二次代价函数中求$\sigma \prime(z)$导致学习缓慢的问题。类似的方法，我们可以计算出关于偏置的偏导数： \frac{\partial C}{\partial b} = \frac{1}{n} \sum_{x} (\sigma(z) - y)那么我们应该在什么时候用交叉熵来替换⼆次代价函数？实际上，如果输出神经元是S型神经元(如sigmoid函数)时，交叉熵一般都是更好的选择。 注意： 对其他的问题（如回归问题）y 可以取0和1之间的中间值的。其实，交叉熵对所有训练输入在$\sigma(z) = y$时仍然是最小化的。此时交叉熵的表示是：C = -\frac{1}{n}\sum_{x}[y \ln y + (1-y)\ln(1-y)]其中$-[y \ln y + (1-y)\ln(1-y)]$有时候被称为二元熵。 交叉熵避免了学习缓慢的问题，不仅仅是在一个神经元上，且在多层多元网络上都起作。这个分析过程稍作变化对偏置也是一样的。 如果输出神经元是线性的，不再是S型函数作用的结果，那么二次代价函数不再会导致学习速度下降的问题。在此情形下，二次代价函数就是一种合适的选择。 使用交叉熵来对MNIST数字进行分类：12345678910import mnist_loader&gt;&gt;&gt; training_data, validation_data, test_data = mnist_loader.load_data_wrapper()&gt;&gt;&gt; import network2&gt;&gt;&gt; net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)&gt;&gt;&gt; net.large_weight_initializer()# 学习30次迭代期，小批量数据大小为10，学习速率 = 0.5&gt;&gt;&gt; net.SGD(training_data, 30, 10, 0.5, evaluation_data=test_data,monitor_evaluation_accuracy=True)# 最高准确率：# Epoch 26 training complete# Accuracy on evaluation data: 9550 / 10000 softmaxsoftmax的想法其实就是为神经网络定义一种新式的输出层。开始时和S型层一样的，首先计算带权输入z，不过不会使用S型函数作用获得输出，而是用一个叫softmax的函数作用在z上获得输出，根据这个函数，第j个神经元的输出就是： a_j^L = \frac{e^{z_j^L}}{\sum_k e^{z_j^L}}其中，分母中的求和是在所有输出神经元上进行的，不难看出，输出激活值都是正数，而且激活值加起来正好为1。 因此，softmax层的输出可以被看做是一个概率分布。这样的效果很令人满意，在很多问题中，能够将输出激活值a理解为网络对于正确输出为的概率的估计是非常方便的。 softmax的一些性质： 具有单调性。 非局部性，任何特定的输出激活值依赖所有带权输入。 作为一种更加通用的视角，softmax + 对数似然代价函数的组合，更加适用于那些需要将输出激活值解释为概率的场景。 过拟合和规范化过度拟合是神经网络的一个主要问题。这在现代网络中特别正常，因为网络权重和偏置数量巨大。为了高效地训练，我们需要一种检测过度拟合是不是发生的技术，这样我们不会过度训练，并且我们也想要找到一些技术来降低过度拟合的影响。看一个例子：1234567&gt;&gt;&gt; import mnist_loader&gt;&gt;&gt; training_data, validation_data, test_data = mnist_loader.load_data_wrapper()&gt;&gt;&gt; import network2&gt;&gt;&gt; net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)&gt;&gt;&gt; net.large_weight_initializer()# 学习400次迭代期，小批量数据大小为10，学习速率 = 0.5，只用1000幅图像&gt;&gt;&gt; net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data, monitor_evaluation_accuracy=True, monitor_training_cost=True) 我们可以画出当网络学习时代价变化的情况：以及分类准确率在测试集上的表现：可以看出，网络在280迭代期后就过度拟合（overfitting）或者过度训练了。 检测过度拟合的明显方法是—— 跟踪测试数据集合上的准确率随训练变化情况。如果我们看到测试数据上的准确率不再提升，那么我们就停止训练。使用验证集检验模型是否过拟合是一般的做法。 L2规范化一般来说，最好的降低过拟合的方式之一是增加训练样本量，有了足够的训练数据，就算是一个规模非常大的网络也不太容易过拟合，但不幸的是，训练数据其实是很难或者很昂贵的资源，所以这不是一种太切合实际的选择。 其实，还有其他的技术能够缓解过拟合，即使我们只有一个固定的网络和固定的训练集合，而这种技术就是规范化。最为常见的规范化手段被称为权重衰减或者L2规范化。L2 规范化的想法是增加⼀个额外的项到代价函数上，这个项叫做规范化项。下面是规范化的交叉熵代价函数： C = -\frac{1}{n}\sum_{x_j}[y_j \ln a_j^L + (1-y_j)\ln(1-a_j^L)]+\frac{\lambda}{2n}\sum_w w^2其中，第一项就是常规的交叉熵表达式，第二项加入的就是所有权重的平方的和，然后使用一个因子进行量化调整，其中 $\lambda&gt; 0$ 可以称为规范化参数。（当然，对其他的代价函数也可以进⾏规范化）。都可以写成以下形式： C=C_0 + \frac{\lambda}{2n}\sum_w w^2现在，对于这样的规范化为何能够减轻过度拟合还不是很清楚，首先我们需要知道如何计算对网络中所有权重和偏置的偏导数： \frac{\partial C}{\partial w} = \frac{\partial C_0}{\partial w} + \frac{\lambda}{n} w\frac{\partial C}{\partial b} = \frac{\partial C_0}{\partial b}权重的学习规则就变成： w = w - \eta \frac{\partial C_0}{\partial w} - \frac{\eta \lambda}{n} w = (1- \frac{\eta \lambda}{n}) w - \eta \frac{\partial C_0}{\partial w}这正和通常的梯度下降学习规则相同，除了通过一个因子$1- \frac{\eta \lambda}{n}$重新调整了权重w。这种调整有时被称为权重衰减，因为它使得权重变小了。通常小的权重在某种程度上，意味着更低的复杂性，也就对数据给出了一种更简单却更强大的解释，因此应该优先选择。 规范化交叉熵代价函数的例子：1234567&gt;&gt;&gt; import mnist_loader&gt;&gt;&gt; training_data, validation_data, test_data = mnist_loader.load_data_wrapper()&gt;&gt;&gt; import network2&gt;&gt;&gt; net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)&gt;&gt;&gt; net.large_weight_initializer()# 30 个隐藏神经元、小批量数据大小为10，学习速率为0.5，400个迭代期，规范化参数lmbda=0.1&gt;&gt;&gt; net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data, lmbda = 0.1, monitor_evaluation_cost=True, monitor_evaluation_accuracy=True, monitor_training_cost=True, monitor_training_accuracy=True) 训练集上的代价函数持续下降：测试集上的准确率在整个400迭代期内持续增加：显然，规范化的使.能够解决过度拟合的问题。而且，准确率相当高了，相较于之前的82.27%，最高处达到了87.1%。 规范化的的其他技术除了L2 外还有很多规范化技术。实际上，正是由于数量众多，也不会将所有的都列举出来。在本节，简要地给出三种减轻过度拟合的其他的方法：L1 规范化、弃权和人为增加训练样本。 L1 规范化： 这个方法是在未规范化的代价函数上加上一个权重绝对值的和： C = C_0 + \frac{\lambda}{n} \sum_w |w|对应*L1 规范化代价函数的偏导数有： \frac{\partial C}{\partial w} = \frac{\partial C_0}{\partial w} + \frac{\lambda}{n} sgn(w)其中sgn(w) 就是w 的正负号，即w是正数时为+1，w为负数时为-1。使用这个表达式，我们可以轻易地对反向传播进行修改，从而使用基于L1规范化的随机梯度下降进行学习。对L1规范化的网络进行更新的规则就是： w = w - \eta \frac{\partial C_0}{\partial w} - \frac{\eta \lambda}{n} sgn(w)L1和L2两种情形下，规范化的效果就是缩小权重。这符合我们的直觉，两种规范化都惩罚大的权重。但权重缩小的方式不同。在L1规范化中，权重通过一个常量向0进行缩小。在L2规范化中，权重通过一个和w成比例的量进行缩小的。 所以，当一个特定的权重绝对值|w|很大时，L1规范化的权重缩小得远比L2规范化要小得多。相反，当一个特定的权重绝对值|w|很小时，L1规范化的权重缩小得要比L2规范化大得多。最终的结果就是，L1规范化倾向于聚集网络的权重在相对少量的高重要度连接上，而其他权重就会被驱使向0接近。 注意：我在上面的讨论中其实忽略了一个问题—— 在w = 0 的时候，w偏导数未定义义。原因在于函数|w|在w = 0 时有个“直角”，事实上，导数是不存的。不过没关系，我们要做的就是应用通常的（无规范化的）随机梯度下降的规则在w = 0处，更确切地说，我们约定sgn(0) = 0。 弃权： 弃权（Dropout）是一种相当激进的技术。和L1、L2规范化不同，弃权技术并不依赖对代价函数的修改。而是，在弃权中，我们改变了网络本身，先描述一下弃权基本的工作机制。我们尝试训练一个网络： 假设我们有一个训练数据x和对应的目标输出y。通常我们会通过在网络中前向传播x，然后进行反向传播来确定对梯度的贡献。使用弃权技术，这个过程就改了。我们会从随机（临时）地删除网络中的一半的隐藏神经元开始，同时让输入层和输出层的神经元保持不变。在此之后，我们会得到最终如下线条所示的网络。注意那些被弃权的神经元，即那些临时被删除的神经元，用虚圈表示在图中： 我们前向传播输入x，通过修改后的网络，然后反向传播结果，在一个小批量数据样本上进行这些步骤后，对有关的权重和偏置进行更新，重置弃权的神经元，然后重复上述过程，最终网络会学到一个权重和偏置的集合。当然，这些权重和偏置也是在一半的隐藏神经元被弃权的情形下学到的。当实际运行整个网络时，是指两倍的隐藏神经元将会被激活。为了补偿这个，我们将从隐藏神经元出去的权重减半。 为什么弃权的方法能够进行规范化呢？启发式地看，当我们弃权掉不同的神经元集合时，有点像我们在训练不同的神经网络。所以，弃权过程就如同大量不同网络的效果的平均那样。不同的网络会以不同的方式过度拟合了，所以，弃权过的网络的效果会减轻过度拟合。弃权技术在训练大规模深度网络时尤其有效，这样的网络中过度拟合问题经常特别突出。 人为扩展训练数据：获取更多的训练样本其实是很好的想法。不幸的是，这个方法代价很.，在实践中常常是很难达到的。不过，还有一种方法能够获得类似的效果，那就是人为扩展训练数据，比如，旋转、转换、扭曲图像，对于语音识别，可以通过增加背景噪声来扩展数据等。 一般而言，准确率会随着更多的数据而不断增加。当然，在训练的后期，我们会发现学习已经接近饱和状态。 权重初始化创建了神经网络后，我们需要进行权重和偏置的初始化。一种方式就是根据独立高斯随机变量来选择权重和偏置，其被归一化为均值为0，标准差1。这个方法还不错，但是非常特别，所以值得去重新探讨它，看看是否能寻找一些更好的方式来设置初始的权重和偏置，这也许能帮助我们的网络学习得更快。结果表明，我们可以使用比归一化的高斯分布做得更好的方法。 假设我们使用一个有大量输入神经元的网络，比如说1000个，并且训练输入x，其中一半的输入神经元值为1，另一半为0，已经使用了归一化的高斯分布初始化了连接隐藏层的权重，考虑隐藏神经元输入的带权和： z = \sum_j w_jx_j + b其中500个项消去了，因为对应的输入为0。所以z是遍历总共501个归一化的高斯随机变量的和，包含500个权重项和额外的1个偏置项。因此z本身是一个均值为0，标准差为$\sqrt {501}$ 的高斯分布。即，z是一个非常宽的高斯分布，分布图形完全不是很尖的形状，|z|会变得很大，这样隐藏神经元的输出$\sigma(z)$ 就会接近1或者0，也就表示我们的隐藏神经元会饱和。 所以当出现这样的情况时，在权重中进行微小的调整仅仅会给隐藏神经元的激活值带来极其微弱的改变。而这种微弱的改变也会影响网络中剩下的神经元，然后会带来相应的代价函数的改变。结果就是，这些权重在我们进行梯度下降算法时会学习得非常缓慢。 解决方法：假设我们有一个n个输入权重的神经元，使用均值0，标准差$1/\sqrt n$的高斯分布初始化这些权重，也就是说会向下挤压图形，让神经元更不可能饱和，继续使用均值0标准差1的高斯分布对偏置进行初始化。 继续使用500个值为0的输入和500 个值为1的输入，很容易证明此时隐藏神经元输入的带权和z服从均值0，标准差为$\sqrt {3/2}$ 的高斯分布，这要比以前有更尖锐的峰值。这样的一个神经元更不可能饱和，因此也不太可能遇到学习速度下降的问题。 MNIST 数字分类任务，使用30个隐藏神经元，小批量数据的大小为10，规范化参数5.0，然后是交叉熵代价函数，分别使用新旧权重初始化方法训练：同样的情况使用100个隐藏神经元，得到： 权重初始化不仅仅能够带来训练速度的加快，有时候在最终性能上也有所提升。 如何选择神经网络的超参数直到现在，我们还没有解释对诸如学习速率 $\eta$，规范化参数 $\lambda$ 等等超参数选择的方法。 宽泛策略： 在使用神经网络来解决新的问题时，一个挑战就是获得任何一种非寻常的学习，也就是说，达到比随机的情况更好的结果。这个实际上会很困难，尤其是遇到一种新类型的问题时。现在我们看看一些具体的设置超参数的推荐。 学习速率：假设我们运行了三个不同学习速率（0.025、0.25、2.5）的MNIST 网络，其他参数一致。 我们可以如下设置学习速率。首先，找到学习速率阈值的估计，它使得在训练数据上的代价立即开始下降而非震荡或者增加，这个估计并不需要太过精确，可以估计这个值的量级，比如说从等于0.01开始。 如果代价在训练前面若干回合开始下降，就可以逐步增大学习速率，比如0.1、1.0，直到若干回合代价开始震荡或者增加。相反，如果代价在学习速率等于0.01时就开始震荡或增加，那就减小它，比如0.001、0.0001，直到代价开始下降。 显然， 学习速率实际值不应该比阈值大。实际上，更应该使用稍微小点的值，例如，阈值的一半这样的选择。 为何对学习速率要用训练代价来选择呢，而其他超参数用验证集？原因就是其他的超参数倾向于提升最终的测试集上的分类准确率，所以将他们通过验证准确率来选择更合理一些。然而，学习速率仅仅是偶然地影响最终的分类准确率的，学习速率主要的目的是控制梯度下降的步长，监控训练代价是最好的检测步长过大的方法。 使用提前停止来确定训练的迭代期数量：提前停止表示在每个回合的最后，都要计算验证集上的准确率，当准确率不再上升，就终止它。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-模型评估与选择（1）]]></title>
    <url>%2F2018%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%EF%BC%881%EF%BC%89.html</url>
    <content type="text"><![CDATA[前沿经小组协商，打算分工全方面、系统地学习机器学习的知识点。通过理论结合实践，从而达到对机器学习的本质有更加深入的了解，本篇博文参考《机器学习》西瓜书和李航老师的《统计学方法》，主要关于机器学习之模型评估与评估的一部分知识，后续更进补充。 统计学习三要素统计学习(statistical learning) 是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科，亦称为统计机器学习(statistical machine learning)。统计学习包括监督学习、非监督学习、半监督学习以及强化学习。统计学习方法都是由模型、策略、算法构成的，即统计学习的三要素，可以表示为： 方法 = 模型 + 策略 + 算法 模型：指所要学习的条件概率分布或者决策函数，模型的假设空间包含所有可能的条件概率分布或决策函数。决策函数表示的模型为非概率模型，条件概率表示的模型为概率模型。 策略：有了模型的假设空间，策略就是要考虑按照什么样的准则学习或者选择最优的模型。 损失函数(loss function)或者代价函数(cost function)度量模型预测的好坏，风险函数(risk function)或者期望损失(expected loss)度量平均意义下模型预测的好坏，学习的目标就是选择期望风险最小的模型。模型关于训练数据集的平均损失称为经验风险(empirical risk)或者经验损失(empirical loss)，当样本容量N趋于无穷时，经验风险趋近于期望风险，所以很自然的想法是用经验风险估计期望风险。但是往往现实当中样本数目有限，经验风险最小化(empirical risk minimization，ERM)学习的效果就未必很好，会产生“过拟合(over-fitting)“现象，由此引出结构风险最小化(structural risk minimization，SRM)的策略，结构风险最小化等价于正则化(regularization)，结构风险小需要经验风险与模型复杂度同时小，结构风险小的模型往往对训练数据以及未知的预测数据都有较好的预测。经验风险函数或者结构风险函数就是最优化的目标函数。 算法：从假设空间中选择最优模型，最后需要考虑的是用什么样的计算方法求解最优模型，而算法就指的是学习模型的具体计算方法。 这时，统计学习的问题归结为最优化问题。如果最优化问题有显示的解析解，这个最优化问题就比较简单。但通常解析解不存在，这就需要用数值计算的方法求解。如何保证找到全局最优解，并使得求解过程非常高效，就成为一个重要问题。 模型评估与选择经验误差与过拟合统计学习的目的是使学到的模型不仅对已知数据而且对未知数据都能有很好的预测能力。不同的学习方法会给出不同的模型，当损失函数给定时，基于损失函数的模型的训练误差和模型的测试误差就自然成为学习方法的评估标准。模型在训练集上的误差称为训练误差或者经验误差，在新样本上的误差称为测试误差或者泛化误差。显然，我们希望得到泛化误差小的学习器。 我们事先不知道新样本是什么样，实际能做的就是努力使经验误差最小化，为了达到这个目的，应该从训练样本中尽可能学出适用于所有潜在样本的”普遍规律”。在很多情况下，我们确实可以学得一个在训练集上表现很好的学习器，精度甚至可以达到100%，遗憾的是，这样的学习器在大多数情况下都不好。 当模型把训练样本学得”太好”的时候，很可能已经把训练样本本身的一些特点当做了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降，这种现象成为过拟合。 过拟合：也可以理解为在学习时选择的模型包含的参数过多，以致于出现对已知的数据预测得很好，但对未知数据预测得很差的现象。 评估方法通常，我们可以通过实验测试来对模型的泛化误差进行评估，进而做出选择。为此，需要一个”测试集”来测试模型对新样本的判别能力，然后以测试集上的测试误差作为泛化误差的近似。通常我们假设测试样本也是从样本真实分布中独立同分布采样得到，但测试集应该尽可能与训练集互斥。一个只有m个样本的数据集D，既要训练，又要测试，那就要对D进行适当的划分处理，下面介绍几种方法： 留出法(hold-out)直接将数据集D划分为两个互斥的集合，其中一个作为训练集S，另一个作为测试集T。在S上训练模型后，用T来评估测试误差，作为对泛化误差的估计。需要注意的三点： 训练/测试集的划分要尽可能保持数据分布的一致性，例如在分类问题中至少要保持样本的类别比例相似（分层采样）。 即使给出划分比例，划分的结果也有多种，一般要采用若干次随机划分、重复进行实验评估后取平均值。 测试集小的话，评估结果的方差较大；训练集小的话，评估结果的偏差较大。所以通常将大约2/3~4/5的样本用于训练，剩余样本用于测试。 交叉验证法(cross validation)将数据集D划分为k个大小相似的互斥子集，每个子集尽可能保持数据分布一致性（分层采样）。每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，从而可以用k组训练/测试集分别进行训练和测试，得到k个测试结果的均值，该过程称为k折交叉验证(k-fold cross validation)。k通常取值为10，即10折交叉验证。 同样，为避免划分不同引入的差别，k折交叉验证通常也要随机使用不同的划分重复p次，最终的评估结果是p次k折交叉验证结果的均值。 特例，留一法(leave one out，LOO)：测试集只包含一个样本，优点是偏差小，评估结果往往被认为比较准确，缺点是模型过多，计算开销太大。 12345678910111213# k折交叉验证import numpy as npfrom sklearn.model_selection import KFoldX = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])y = np.array([0, 1, 0, 1])X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]kf = KFold(n_splits=2) # k = 2for train, test in kf.split(X): print("%s %s" % (train, test)) X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]&gt;&gt;&gt; out:[2 3] [0 1][0 1] [2 3] 1234567891011# 留一法from sklearn.model_selection import LeaveOneOutX = [1, 2, 3, 4]loo = LeaveOneOut()for train, test in loo.split(X): print("%s %s" % (train, test))&gt;&gt;&gt; out:[1 2 3] [0][0 2 3] [1][0 1 3] [2][0 1 2] [3] 1234567891011#分层k折交叉验证from sklearn.model_selection import StratifiedKFoldX = np.ones(10)y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]skf = StratifiedKFold(n_splits=3) # k = 3for train, test in skf.split(X, y): print("%s %s" % (train, test))&gt;&gt;&gt; out:[2 3 6 7 8 9] [0 1 4 5][0 1 3 4 5 8 9] [2 6 7][0 1 2 4 5 6 7] [3 8 9] 自助法(bootstrapping)在留出法和交叉验证法中，都保留了一部分样本用于测试，这必然会引入因训练规模不同而导致的偏差，自助法可以避免这个问题。 自助法以自助采样为基础。给定包含m个样本的数据集D，每次从D中挑选一个样本，将其拷贝放入D’中，然后放回D，使得该样本下次还有被挑中的可能；重复该过程m次，我们就得到了m个样本的数据集D’。显然D中有一部分样本会在D’中多次出现，而另一部分样本不会出现。统计可知，初始数据集D中约有36.8%的样本未出现在D’中。D’当作训练集（m个样本），D/D’用作测试集。 在数据集较小、难以有效划分训练/测试集时，自助法很有用。 自助法可以通过多次采样产生多个不同的训练集，一般用于集成学习。 自助法产生的数据集改变了初始数据集的分布，这会引入偏差，因此，数据量大时，一般使用留出法和交叉验证法。 调参与最终模型大多数学习算法都有参数(parameter)需要设定，在模型评估与选择时候，除了要对算法进行选择，还要对算法参数进行设定，即为”调参(parameter tunning)”。 学习算法的参数在实数范围内取值的，选参通常是对每个参数选定一个范围和变化步长，如在[0，0.2]区间内以0.05为步长，则候选参数为5个。 在模型选择完成后，学习算法和参数就已经选定了，此时应该用全量数据集重新训练一遍模型，这才是最终的模型。 通常把模型在实际应用中遇到的数据称为测试数据，而模型评估与选择中用于评估测试的数据集称为”验证集(validation set)”。即，测试集上的效果判别模型的泛化能力，训练数据划分为训练集和验证集，基于验证集上的效果来进行模型选择和调参。 性能评估对模型的评估，不仅需要有效的估计方法，还需要有衡量模型泛化能力的指标，这就是性能度量(performance measure)。 回归预测常用的性能度量 均方误差(mean squared error) MSE(y, \hat{y}) = \frac{1}{n_{sample}}\sum_{i=0}^{n_{sample}-1}(y_i - \hat{y}_i)^2123456from sklearn.metrics import mean_squared_errory_true = [3, -0.5, 2, 7]y_pred = [2.5, 0.0, 2, 8]mean_squared_error(y_true, y_pred)&gt;&gt;&gt; out:0.375 平均绝对误差(mean absolute error) MAE(y, \hat{y}) = \frac{1}{n_{sample}}\sum_{i=0}^{n_{sample}-1}|y_i - \hat{y}_i|123456from sklearn.metrics import mean_absolute_errory_true = [3, -0.5, 2, 7]y_pred = [2.5, 0.0, 2, 8]mean_absolute_error(y_true, y_pred)&gt;&gt;&gt; out:0.5 解释方差得分(explained variance score) explained-variance(y, \hat{y}) = 1-\frac{Var(y-\hat{y})}{Var(y)}其中，Var是方差，最好的得分是 1，值越低越差。 123456from sklearn.metrics import explained_variance_scorey_true = [3, -0.5, 2, 7]y_pred = [2.5, 0.0, 2, 8]explained_variance_score(y_true, y_pred)&gt;&gt;&gt; out:0.95717344753747324 可决系数(R2 score) R^2(y, \hat{y}) = 1-\frac{\sum_{i=0}^{n_{sample}-1}(y_i - \hat{y}_i)^2}{\sum_{i=0}^{n_{sample}-1}(y_i - \bar{y})^2}其中， \bar{y} = \frac{1}{n_{sample}}\sum_{i=0}^{n_{sample}-1}y_i它提供了将来样本如何可能被模型预测的估量。最佳分数为1，可以为负数（因为模型可能会更糟）。 123456from sklearn.metrics import r2_scorey_true = [3, -0.5, 2, 7]y_pred = [2.5, 0.0, 2, 8]r2_score(y_true, y_pred)&gt;&gt;&gt; out:0.94860813704496794 分类问题常用的性能度量 错误率与精度错误率是分类错误的样本数占总数的比例，精度则是分类正确的样本占总数的比例，即为： acc(y, \hat{y}) = \frac{1}{n_{sample}}\sum_{i=0}^{n_{sample}-1}1(y_i = \hat{y}_i)其中，$1(x)$是0-1指示函数。 12345import numpy as npfrom sklearn.metrics import accuracy_scoreaccuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))&gt;&gt;&gt; out:0.5 查准率、查全率对于二分类问题，可以将样例划分为真正例(true positive)、假正例(false positive)、真反例(true negative)、假反例(false negative)四种情形，令TP、FP、TN、FN分别为其对应的样本数，显然TP + FP + TN + FN = 样本总数。查准率P：在所有预测为正样本的样本(TP+FP)中预测正确(TP)的比例，查全率R：在所有正样本(TP+FN)中，预测正确(TP)的比例。 P = \frac{TP}{TP+FP}R = \frac{TP}{TP+FN}通常，两者往往是一对矛盾的度量，查准率高时，查全率偏低；查全率高时，查准率偏低，只有在一些简单的任务中，两者才可能都高。 1234567# 混淆矩阵from sklearn.metrics import confusion_matrixy_true = [0, 0, 0, 1, 1, 1, 1, 1]y_pred = [0, 1, 0, 1, 0, 1, 0, 1]tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()&gt;&gt;&gt; out:(2, 1, 2, 3) F1分数为了综合考虑查准率与查全率，于是有了F1度量： F1 = \frac{2 * P * R}{P + R} = \frac{2 * TP}{n_{sample} + TP - TN}F1是基于查准率与查全率的调和平均，在一些应用中，对查准率和查全率的重视程度有所不同，因此，有F1加权的一般形式为： F_{\beta} = \frac{(1+\beta^2) * P * R}{(\beta^2 + P) + R}, \beta > 0$\beta$度量了查全率对查准率的相对重要性，$\beta = 1$就是标准的F1；$\beta &gt; 1$时查全率影响更大，反之，查准率影响更大。 1234567891011121314#分类指标报告from sklearn.metrics import classification_reporty_true = [0, 1, 2, 2, 0]y_pred = [0, 0, 2, 1, 0]target_names = ['class 0', 'class 1', 'class 2']print(classification_report(y_true, y_pred, target_names=target_names))&gt;&gt;&gt; out: precision recall f1-score support class 0 0.67 1.00 0.80 2 class 1 0.00 0.00 0.00 1 class 2 1.00 0.50 0.67 2avg / total 0.67 0.60 0.59 5 ROC与AUCROC(receiver operating characteristic)曲线的提出是考虑模型在不同任务下的”期望泛化误差”的好坏。“真正率”(true positive rate, TPR)：正确分类的正样本占所有正样本的比例“假正率”(false positive rate, FPR)：误分类为正样本占所有负样本的比例。 TPR = \frac{TP}{TP+FN}FPR = \frac{FP}{FP+TN}ROC曲线就是当将预测结果分割为正负样本的阈值变化时，以FPR作为x轴，以TPR作为y轴得到的曲线，ROC曲线下的面积即为AUC(area under ROC curve)，如下图基于有限预测样本的图： ROC曲线中的四个点和一条线:(0,1)：即FPR=0，TPR=1，FN＝FP＝0，将所有的样本都正确分类；(1,0)：即FPR=1，TPR=0，最差分类器，避开了所有正确答案；(0,0)：即FPR=TPR=0，FP＝TP＝0，分类预测所有的样本都为负样本；(1,1)：即FPR=TPR=1，FN=TN＝0，分类预测所有的样本都为正样本。总之：ROC曲线越接近左上角，该分类器的性能越好。AUC的情况：AUC = 1：绝对完美分类器，理想状态下，100%完美识别正负类；0.5&lt;AUC&lt;1：优于随机猜测。这个分类器妥善设定阈值的话，可能有预测价值；AUC=0.5：跟随机猜测一样，模型没有预测价值；AUC&lt;0.5：比随机猜测还差；只要反预测就优于随机猜测，不存在该状况。总之：AUC值越大的分类器，正确率越高。 1234567891011121314151617181920# ROCimport numpy as npfrom sklearn.metrics import roc_curvey = np.array([1, 1, 2, 2])scores = np.array([0.1, 0.4, 0.35, 0.8])fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)&gt;&gt;&gt; fprarray([ 0. , 0.5, 0.5, 1. ])&gt;&gt;&gt; tprarray([ 0.5, 0.5, 1. , 1. ])&gt;&gt;&gt; thresholdsarray([ 0.8 , 0.4 , 0.35, 0.1 ])# AUCimport numpy as npfrom sklearn.metrics import roc_auc_scorey_true = np.array([0, 0, 1, 1])y_scores = np.array([0.1, 0.4, 0.35, 0.8])roc_auc_score(y_true, y_scores)&gt;&gt;&gt; out:0.75 既然已经这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反） 对数损失对数损失(Log loss)亦被称为逻辑回归损失(Logistic regression loss)或交叉熵损失(Cross-entropy loss)。对于具有真实标签 $y \in {0,1}$ 的二分类和概率估计$p = \rm{Pr}(y = 1)$， 每个样本的log loss是给定的分类器的negative log-likelihood真正的标签: L_{\log}(y, p) = -\log \rm{Pr}(y|p) = -(y\log (p) + (1 - y) \log (1 - p)) 铰链损失铰链损失(hinge loss)函数使用hinge loss计算模型和数据之间的平均距离，这是一种只考虑预测误差单向指标，用于最大边界分类器，如支持向量机。铰链损失最开始出现在二分类问题中，假设正样本被标记为1，负样本被标记为-1，$y$是真实值，$w$是预测值，则铰链损失定义为： L_{\text{Hinge}}(w, y)=\max\{1-wy,0\}=|1-wy|_+然后被扩展到多分类问题，假设$y_w$是对真实分类的预测值，$y_t$是对非真实分类预测中的最大值，则铰链损失定义为： L_{\text{Hinge}}(y_w, y_t)=\max\{1+y_t-y_w,0\} 海明距离海明距离(Hamming Distance)用于需要对样本多个标签进行分类的场景。对于给定的样本$i$，$L$是标签数量，则第$i$个样本的的海明距离为： D_{Hamming}(\hat{y}_i,y_i)=\frac{1}{L}\sum\limits_{j=1}^L 1(\hat{y}_{ij}\neq y_{ij})其中$1(x)$是指示函数。当预测结果与实际情况完全相符时，距离为0；当预测结果与实际情况完全不符时，距离为1；当预测结果是实际情况的真子集或真超集时，距离介于0到1之间。我们可以通过对所有样本的预测情况求平均，得到算法在测试集上的总体表现情况，当标签数量$L$为1时，它等价于精度(Accuracy)，当标签数$L&gt;1$时也有较好的区分度，不像准确率那么严格。 杰卡德相似系数杰卡德相似系数(Jaccard similarity coefficients)也是用于需要对样本多个标签进行分类的场景。对于给定的样本$i$，$\hat{y}_i$是预测结果，${y}_i$是真实结果，$L$是标签数量，则第$i$个样本的杰卡德相似系数为： J(\hat{y}_i,y_i)=\frac{|\hat{y}_i\bigcap y_i|}{|\hat{y_i}\bigcup y_i|}它与海明距离的不同之处在于分母。当预测结果与实际情况完全相符时，系数为1;当预测结果与实际情况完全不符时，系数为0;当预测结果是实际情况的真子集或真超集时，距离介于0到1之间。同样可以通过对所有样本的预测情况求平均得到算法在测试集上的总体表现情况，当标签数量$L$为1时，它等价于精度(Accuracy)。 后续检验方法，未待完续...]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Model selection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信贷评分卡模型]]></title>
    <url>%2F2018%2F04%2F%E4%BF%A1%E8%B4%B7%E8%AF%84%E5%88%86%E5%8D%A1%E6%A8%A1%E5%9E%8B.html</url>
    <content type="text"><![CDATA[简介评分卡是综合个人客户的多个维度信息（如基本情况、偿债能力、信用状况等，重点关注借贷意愿、偿债能力、还款意愿），基于这些信息综合运用数学分析模型，给个人综合评分，判断违约的可能性的工具。信用评分卡模型在国外是一种成熟的预测方法，尤其在信用风险评估以及金融风险控制领域更是得到了比较广泛的使用，其原理是将模型变量WOE编码方式离散化之后运用logistic回归模型进行的一种二分类变量的广义线性模型。（就分析方法发而言，现在分类算法有很多种，决策树，逻辑回归，支持向量机，神经网络等等，都可以实现这个目的）。 评分卡是什么样子的？芝麻信用的评分： 一个典型的贷前审批评分卡：那么，问题来了：所以，评分卡设计围绕三个问题开展： 如何对年龄、收入这样的连续变量进行分组？ 有了分组的结果后，每个分组应该给多少信用分？ 如何定义一个合适的授信门槛分数？ 基于Logistic回归的标准评分卡由逻辑回归的基本原理，我们将客户违约的概率表示为p，则正常的概率为1-p。因此，可以得到违约和不违约的概率为： Odds = \frac{p}{1-p}其中， p(y=1\mid x) = \frac{1}{1+ \rm e^{-g(x)}}p(y=0\mid x) = 1- p(y=1\mid x) = \frac{1}{1+ \rm e^{g(x)}}客户的违约概率也可以表示为： p = \frac{Odds}{1+Odds}评分卡设定的分值刻度可以通过将分值表示为比率对数的线性表达式来定义，即可表示为下式： Score = A-B*\log(Odds)\log(Odds) = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \cdots + \beta_{n}x_{n}常数A、B的值可以通过将两个已知或假设的分值带入计算得到。通常情况下，需要设定两个假设：（1）给某个特定的比率设定特定的预期分值；（2）确定比率翻番的分数（PDO） P = A-B*\log(x)P - PDO = A-B*\log(2*x)评分卡刻度参数A和B确定以后，就可以计算比率和违约概率，以及对应的分值了。通常将常数A称为补偿，常数B称为刻度。 评分卡的分值可表达为： Score = A - B( \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \cdots + \beta_{n}x_{n})所有变量都用WOE进行转换，可得到： Score = (A - B\beta_{0})-(B\beta_{1}w_{11})\delta_{11}-(B\beta_{1}w_{12})\delta_{12}- \cdots \\ -(B\beta_{x}w_{x1})\delta_{x1} - (B\beta_{x}w_{x2})\delta_{x2} - \cdots式中wij为第i个变量第j个值的WOE，为已知变量；betai为逻辑回归方程中的系数，为已知变量；deltaij为二元变量，表示变量i是否取第j个值。此式即为最终评分卡公式。如果变量取不同行并计算其WOE值，式中表示的标准评分卡格式，如下表所示： WOE 和 IV 详解WOEWOE（Weight of Evidence），即证据权重。WOE是对原始自变量的一种编码形式。要对一个变量进行WOE编码，需要首先把这个变量进行分组处理（也叫离散化、分箱等等）。 WOE_{i} = \ln(\frac{py_{i}}{pn_{i}}) = \ln(\frac{\#y_{i}/\#y_{T}}{\#n_{i}/\#n_{T}})其中，pyi是这个组中响应客户（风险模型中，对应的是违约客户，总之，指的是模型中预测变量取值为“是”或者说1的个体）占所有样本中所有响应客户的比例，pni是这个组中未响应客户占样本中所有未响应客户的比例，#yi是这个组中响应客户的数量，#ni是这个组中未响应客户的数量，#yT是样本中所有响应客户的数量，#nT是样本中所有未响应客户的数量。 从公式中可知，WOE表示的实际上是“当前分组中响应客户占所有响应客户的比例”和“当前分组中没有响应的客户占所有没有响应的客户的比例”的差异。对这个公式做一个简单变换，可以得到： WOE_{i} = \ln(\frac{py_{i}}{pn_{i}}) = \ln(\frac{\#y_{i}/\#y_{T}}{\#n_{i}/\#n_{T}}) \\ = \ln(\frac{\#y_{i}/\#n_{i}}{\#y_{T}/\#n_{T}})变换以后我们可以看出，WOE也可以表示的是当前这个组中响应的客户和未响应客户的比值，和所有样本中这个比值的差异。这个差异是用这两个比值的比值，再取对数来表示的。WOE越大，差异越大，这个分组里的样本响应的可能性就越大，WOE越小，差异越小，这个分组里的样本响应的可能性就越小。我们可以看一下WOE的基本特点： 当前分组中，响应的比例越大，WOE值越大； 当前分组WOE的正负，由当前分组响应和未响应的比例，与样本整体响应和未响应的比例的大小关系决定，当前分组的比例小于样本整体比例时，WOE为负，当前分组的比例大于整体比例时，WOE为正，当前分组的比例和整体比例相等时，WOE为0。 WOE的取值范围是全体实数。 WOE 和 OR的关系OR，即Odds Ratio，是两组Odds的比值，比如age1和age2的Odds Ratio为： OR_{age2/age1} = \frac{Odds_{age2}}{Odds_{age1}} = \frac{p(y=1\mid Age = age2) / p(y=0\mid Age = age2)}{p(y=1\mid Age = age1) / p(y=0\mid Age = age1)}OR和Odds在logistic回归中非常值得重视，因为他们和参数的interpretation有关系，并且，当xi增加一个单位时，Odds将发生相应倍数增长： OR_{x_{i}+1/x_{i}} = \frac{Odds_{2}}{Odds_{1}} = \frac{p_{2} / 1-p_{2}}{p_{1} / 1-p_{1}}=\frac{\rm e^{\sum_{j \ne i}\beta_{j}x_{j} + \beta_{i}(x_{i}+1)}}{\rm e^{\sum_{j \ne i}\beta_{j}x_{j} + \beta_{i}x_{i}}}=\rm e^{\beta_{i}}通过条件概率公式转化可知： \ln\frac{p(y=1\mid age_{i})}{p(y=0\mid age_{i})} = \ln\frac{p(y=1)}{p(y=0)} + \ln\frac{p(age_{i}\mid y=1)}{p(age_{i}\mid y=0)}即： \ln(Odds_{age_{i}}) = 常数 + WOE_{age_{i}}因此，我们可以得到OR和WOE的关系为： \ln(OR_{age2/age1}) = WOE_{age2}- WOE_{age1}结论就是：如果使用了WOE编码，当我们对单变量进行回归（Y~Xi）时，可以不做dummy encoding，此时变量的系数恒为1。WOE编码起到了把回归系数“正则化”的作用。下面用实例证明OR与WOE的关系。WOE:12345678import pandas as pdimport numpy as npdf = pd.DataFrame(&#123;'X1':np.random.randint(3,size=1000),'y':np.random.randint(2,size=1000)&#125;)table = pd.crosstab(df['y'],df['X1'])woe_table = table.div(table.sum(axis=1),axis=0)woe = (woe_table.iloc[1,:] / woe_table.iloc[0,:]).apply(lambda x:np.log(x))print(woe[1]-woe[0])print(woe[2]-woe[1]) OR:1234OR_0_1 = np.log(1.0*table.iloc[0,0] * table.iloc[1,1] / table.iloc[0,1] / table.iloc[1,0])OR_1_2 = np.log(1.0*table.iloc[0,1] * table.iloc[1,2] / table.iloc[0,2] / table.iloc[1,1])print(OR_0_1)print(OR_1_2) IV的计算IV（Information Value），中文意思是信息价值，或者信息量。我们在用逻辑回归、决策树等模型方法构建分类模型时，经常需要对自变量进行筛选。那么我们怎么去挑选入模变量呢？需要考虑的因素很多，比如：变量的预测能力，变量之间的相关性，变量的简单性（容易生成和使用），变量的强壮性（不容易被绕过），变量在业务上的可解释性（被挑战时可以解释的通）等等。但是，其中最主要和最直接的衡量标准是变量的预测能力。我们需要一些具体的量化指标来衡量每自变量的预测能力，并根据这些量化指标的大小，来确定哪些变量进入模型。IV就是这样一种指标，它可以用来衡量自变量的预测能力。类似的指标还有信息增益、基尼系数等等。基于上面的WOE，对于分组i，也会有一个对应的IV值，计算公式如下： IV_{i} = (py_{i}-pn_{i}) * WOE_{i} = (\#y_{i}/\#y_{T} - \#n_{i}/\#n_{T})\ln(\frac{\#y_{i}/\#y_{T}}{\#n_{i}/\#n_{T}})有了一个变量各分组的IV值，我们就可以计算整个变量的IV值，方法很简单，就是把各分组的IV相加： IV = \sum_{i}^{n} IV_{i}我们可以看出IV的以下特点： 对于变量的一个分组，这个分组的响应和未响应的比例与样本整体响应和未响应的比例相差越大，IV值越大，否则，IV值越小； 极端情况下，当前分组的响应和未响应的比例和样本整体的响应和未响应的比例相等时，IV值为0； IV值的取值范围是$[0,+\infty)$，且，当当前分组中只包含响应客户或者未响应客户时，IV = $+\infty$。 实际应用中，通过IV值评判特征预测能力的标准如下： IV的极端情况以及处理方式IV依赖WOE，并且IV是一个很好的衡量自变量对目标变量影响程度的指标。但是，使用过程中应该注意一个问题：变量的任何分组中，不应该出现响应数=0或非响应数=0的情况。原因很简单，当变量一个分组中，响应数 = 0时： WOE_{i} = \ln(\frac{0/\#y_{T}}{\#n_{i}/\#n_{T}}) = -\infty而此时，$IV_{i}$为$+\infty$,而当变量一个分组中，没有响应的数量 = 0时： WOE_{i} = \ln(\frac{\#y_{i}/\#y_{T}}{0/\#n_{T}}) = +\infty此时的$IV_{i}$也为$+\infty$，无论IV等于负无穷还是正无穷，都是没有意义的。由上述问题我们可以看到，使用IV其实有一个缺点，就是不能自动处理变量的分组中出现响应比例为0或100%的情况。那么，遇到响应比例为0或者100%的情况，我们应该怎么做呢？建议如下：（1）如果可能，直接把这个分组做成一个规则，作为模型的前置条件或补充条件。（2）重新对变量进行离散化或分组，使每个分组的响应比例都不为0且不为100%，尤其是当一个分组个体数很小时（比如小于100个），强烈建议这样做，因为本身把一个分组个体数弄得很小就不是太合理。（3）如果上面两种方法都无法使用，建议人工把该分组的响应数和非响应的数量进行一定的调整。如果响应数原本为0，可以人工调整响应数为1，如果非响应数原本为0，可以人工调整非响应数为1。 WOE和IV的意义何在？变量替换为WOE值意义 对自变量进行分档，每档一个值来代表这段的自变量输入，通过转换后实现了稳定性要求； 克服不同变量间刻度不统一的问题，克服回归中缺失值的填充问题，很多极值变量通过WOE可以变为非异常值； WOE能反映自变量的贡献情况，结合模型拟合出的系数和自变量内部WOE值的波动情况，构造出各个自变量的贡献率及相对重要性。一般地，系数越大，WOE的方差越大，则自变量的贡献率越大（类似于某种方差贡献率）。 WOE与违约概率具有某种线性关系，提升模型的预测效果，提高模型的可理解性IV意义 衡量自变量的预测能力，从而进行特征选择。 讨论评分卡是舶来品，对信用基础数据有一定要求，信用评分的建模过程更多地是分析的过程（而不是模型拟合的过程），也正因此，对模型参数的估计等等内容似乎并不做太多的学习，而把主要的精力集中于研究各个自变量与目标变量的关系，在此基础上对自变量做筛选和编码，最终再次评估模型的预测效果，并且对模型的各个自变量的效用作出相应的评价。WOE 和 IV应用有局限性，这两个值的资料都是信用评分模型，机器学习的相关资料中很少关于这两个值的应用。虽然网上到处都是神经网络、xgboost的文章，但当下的建模过程中（至少在金融风控领域）并没有完全摆脱logistic模型，原因大致有以下几点：（1） logistic模型客群变化的敏感度不如其他高复杂度模型，因此稳健更好，鲁棒性更强。（2）模型直观。系数含义好阐述、易理解。对金融领域高管以及银行出身的建模专家，变量系数可以跟他们的业内知识做交叉验证，更容易让人信服。（3） 也是基于2的模型直观性，当模型效果衰减的时候，logistic模型能更好的诊断病因。 参考资料：https://www.jianshu.com/p/9eea3e8828b3https://blog.csdn.net/kevin7658/article/details/50780391]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Score card</tag>
        <tag>Logistic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习基础 - 从人工神经网络开始]]></title>
    <url>%2F2017%2F08%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E7%A1%80-%E4%BB%8E%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BC%80%E5%A7%8B.html</url>
    <content type="text"><![CDATA[前言 深度学习（Deep Learning）目前是人工智能领域的大杀器，然而对于大多数研究者来说，对于深度神经网络的理解并不是很透彻，往往视其为黑盒子般的存在，包括本人，也只是知其然却不知其所以然。于是乎，从细节出发，本人决定好好探索深度学习中的奥妙。本文摘自analyticsvidhya上的一篇好文，该文对于从神经网络到深度学习的关键点有很细致、到位的理解。 介绍 深度学习（DL）和神经网络（NN）正推动着当今世纪的一些最具独创性的发明。它们难以置信的从数据和环境中学习的能力，使之成为科学家的首选。深度学习和神经网络是自动驾驶汽车，图像识别软件，推荐系统等产品的核心。显然，作为一种强大的算法，它对各种数据类型也具有高度的自适应性。 目录 什么是神经网络？ 单神经元如何工作？ 为什么多层网络有用？ 神经网络的一般结构 反向传播（非常重要） 什么是神经网络？神经网络（NN）也被称为人工神经网络，是以人类神经系统运转的人工表示而命名的。首先我们来了解下神经系统是如何工作的，神经系统由数以百万计的神经细胞或神经元组成，神经元具有以下结构： 它的主要组件包括： 树突 - 它以电脉冲的形式从其他神经元获取输入。 细胞体 - 它从这些输入生成推论并决定采取什么行动 。 轴突终端 - 它以电脉冲的形式进行传输。 简而言之，每个神经元都通过树突从许多其他神经元接受输入。然后，对输入执行所需的处理，并通过轴突将另一个电脉冲发送到终端节点，从那里又传送给许多其他神经元。 ANN的工作方式非常相似。神经网络的一般结构如下所示： 上图红色框中单独描绘了单一神经元的典型神经网络结构。让我们来理解这一点，每个神经元的输入都是树突，就像在人类的神经系统中一样，一个神经元（虽然是人为的）集结所有的输入并对它们进行操作处理。然后，将输出传送给它所连接的其他神经元（即下一层）。神经网络层次分为3种类型： 输入层：通过将观察样本传入到神经元。 隐藏层：这是输入层和输出层之间的中间层，帮助神经网络学习数据中涉及的复杂关系。 输出层：最后的输出是从前两层提取的。例如：对于一个5个类的分类问题，输出将会有5个神经元。 我们先通过例子来看看每个神经元的功能。 单神经元如何工作？在本小节，透过简单的例子来了解单个神经元的工作机制，从而直观地理解神经元是如何通过使用输入来计算输出。单个神经元的神经网络结构如下：与人类神经系统的神经元不同的是： x1,x2,…,xN：神经元的输入。这些可以是来自输入层的实际观察值或来自某个隐藏层的中间值。 x0：偏置单元。它是一个组成激活函数变量的常量值。 a：神经元的输出，它按照以下公式计算得到：a = f(\sum_{i=0}^N w_ix_i) 这里，f 被称之为激活函数，它使得神经网络结构非常灵活，并且能够探索到数据中复杂的非线性关系。它可以是高斯函数，逻辑函数，双曲函数或甚至是简单的线性函数。 接下来，我们来实现神经网络中的3个基本函数：OR, AND, NOT，这有助于了解它们的内在本质。假设有一个分类问题，可以预测不同输入组合的输出（0或1），我们将使用以下激活函数对线性分类器进行建模： f(x) = \begin{cases} 0, & x < 0 \\ 1, & x >= 0 \end{cases}实例：AND函数AND函数可以这样实现： 该神经元的输出是： a = f(-1.5 + x_1 + x_2)它对应的真值表是： 我们可以看到，AND函数被成功实现了。列’a’和列’X1 AND X2’是一致的。我们注意到此时偏置项权重等于-1.5，但是它不是一个定值，直觉上，我们可以把它赋值为只有当$x_1$和$x_2$都是正数的情况下，并且满足总和为正数情况下的任何值。因此，当它取值（-1，-2）之间时满足要求。 实例：OR函数OR函数可以这样实现： 该神经元对应的输出是： a = f(-0.5 + x_1 + x_2)它对应的真值表为： 列’a’和列’X1 OR X2’是一致的。我们可以看到，通过改变偏置项的权重，就可以实现OR函数了。这个上面的实例很相似，同样，可以直觉地认为偏置项权重取值为当$x_1$或$x_2$至少有一个正值，并满足总和为正数情况下的任何值。 实例：NOT函数同上例一样，NOT函数可以如下被实现： 该神经元对应的输出是： a = f(1 - 2 * x_1)它对应的真值表为： 同样，输出符合NOT函数的表达式，相信通过这些例子，大概可以直观地了解神经网络中的神经元是如何工作的，上面的几个例子都是用了一个简单的激活函数。 注意：上面所用的简单激活函数往往会被替之为逻辑函数，因为逻辑函数可微分并且能求梯度。这是一方面，还有其他，也就是它的输出是浮点值，而不是非0即1。 为什么多层网络有用？理解了单神经元是如何工作之后，让我们继续探索神经网络是怎么通过多层网络结构学习数据间的复杂关系。理解这一点之前，先来看一个 XNOR函数的例子，回顾一下， XNOR函数的真值表如下： 表中可以看出，只有当输入相等时输出才等于1，否则，等于0。这种关系不能用单个神经元建模（不相信？不信试试！），因此接下来我们会用多层网络，使用多层网络的核心思想是复杂关系可以被分解为多个简单的函数。我们来分解下XNOR函数： \begin{aligned} X_1 \ XNOR \ X_2 =& NOT ( X_1 \ XOR \ X_2 ) \\ =& NOT [ (A+B).(A'+B') ] \\ =& (A+B)' + (A'+B')' \\ =& (A'.B') + (A.B) \end{aligned} 注意： ‘+’ 指 OR， ‘.’ 指 AND 现在我们可以使用任何一个简化的例子来实现它。接下来将演示如何使用2个实例去实现XNOR函数。 实例：$X_1 \ XNOR \ X_2 = (A’.B’) + (A.B)$这里的难点就在于怎么设计一个神经元去模拟实现$A’.B’$，可以用以下方式：该神经元对应的输出是： a = f(0.5 - x_1 - x_2)它对应的真值表为： 现在我们已经对各个分函数进行了建模，接下来我们可以使用多层网络对它们进行组合。首先，看看该神经网络结构的语义图： 可以看出，在第1层中，我们将分别实现$A’.B’$和$A.B$，在第2层，我们将使用它们的输出实现一个OR函数。这样将完成整个神经网络过程，最终的网络将如下所示： 如果你仔细观察会发现，上图只不过是我们已经绘制的不同神经元的组合。不同的输出代表不同的单元： $a_1$：实现$A’.B’$ $a_2$：实现$A.B$ $a_2$：实现$a_1$和$a_2$的OR函数，也就是有效实现$ (A’.B’ + A.B)$ 该函数可以使用以下真值表进行验证： 通过上述例子应该大概了解了多层神经网络是如何工作的，我们接下来再实现另一情况。 实例：$X_1 \ XNOR \ X_2 = NOT[(A + B) . (A’ + B’)]$上例中，我们必须单独计算$A’.B’$。如果想直接用基本的AND，OR，NOT函数来实现函数，该怎么办呢？考虑以下语义图： 这里，可以看到我们不得不使用3个隐藏层。和之前的过程类似，该网络看起来像： 这里神经元执行以下操作： $a_1$: 同A一样 $a_2$: 代表A’ $a_3$: 同B一样 $a_4$: 代表A’ $a_5$: 实现OR函数，指的是$A + B$ $a_6$: 实现OR函数，指的是$A’ + B’$ $a_7$: 实现AND函数，指的是$(A + B).(A’ + B’)$ $a_8$: 实现NOT函数，指的是$NOT[(A + B).(A’ + B’)]$，就是最终的XNOR函数 注意，通常情况下，神经元会传送到下一个层的除偏置项之外的其他每个神经元。而在此例中，我已经忽略了从第1层到第2层的几个连接，这是因为它们的权重为0，添加它们会使得在视觉上不太好表达。对应的真值表如下： 最后，我们成功实现了XNOR函数，这种方法比情况1更复杂。因此，应该首选情况1的结构。但这里的核心思想是想展示如何将复杂的函数分解成多个网络层次，于是多层网络的优势在此就凸显出来了。 神经网络的一般结构本节，我们来看一些基本的例子，让我们定义一个神经网络的通用结构。我们还会看到根据给定输入来计算输出的过程，该过程也称为前向传播过程。通用神经网络可以定义为： 它总共有L层，包括1个输入层，1个输出层和L-2个隐藏层。一些术语如下： L：网络的层数 $N_i$：第i层网络的节点数量，除了偏置项，其中i=1,2,…,L $a_i^{(j)}$：第i层网络的第j个节点的输出，其中i=1,2,…,L，j=0,1,2,…$N_i$ 由于每层网络的输出构成了下一层网络的输入，下面我们利用第i层网络的输出来定义第i+1层网络的输出公式。第i+1层网络的输入如下： \begin{aligned} & A_i = [a_i^{(0)}, a_i^{(1)},.....,a_i^{(N_i)}] \\ & 维度：1 * N_i + 1 \end{aligned}从第i层网络到第i+1层网络的权重矩阵为： \begin{aligned} W^{(i)} = [& [W_{01}^{(i)}, W_{11}^{(i)},.....,W_{N_i1}^{(i)}] \\ & [W_{02}^{(i)}, W_{12}^{(i)},.....,W_{N_i2}^{(i)}] \\ & [\ ... ...] \\ & [W_{0N_{i+1}}^{(i)}, W_{1N_{i+1}}^{(i)},.....,W_{N_iN_{i+1}}^{(i)}]] \\ & 维度： N_i + 1 * N_i + 1 \end{aligned}那么。第i+1层网络的输出可以被计算为： \begin{aligned} & A_{i+1} = f(A_i * W^{(i)}) \\ & 维度：1 * N_i + 1 \end{aligned}对于每一层网络使用上述公式，我们就可以得到最终的输出。输出层的节点数量依具体的问题而异，对于回归预测或者二分类问题输出节点可以是1个，对于多分类问题则可以是多个。 上述只是通过一轮迭代得到最终的输出。模型的最终目标则是更新权重，以最大限度地减少损失函数，权重的更新会使用接下来要介绍的反向传播算法。 反向传播（非常重要）反向传播（BP）算法的工作原理是确定输出端的损失（或误差），然后将其传播回网络，更新权重，以最小化每个神经元产生的误差。接下来不会详细介绍算法，但会试着给一些关于它如何运作的例子。 最小化误差的第一步是计算每个节点关于最终输出的梯度。由于这是一个多层的网络，因此计算梯度没那么直接。 我们来了解下多层网络的梯度，抛开神经网络不讲，先考虑一个非常简单的结构，如下所示：该结构有三个输入，满足： d = a - b \\ e = d * c = (a - b) * c现在，我们要求解a,b,c,d关于输出e的梯度，这很直接就能得到： \frac{\partial e}{\partial d} = c \ \frac{\partial e}{\partial c} = d \\ \frac{\partial d}{\partial a} = 1 \ \frac{\partial d}{\partial b} = -1然而，为了计算a,b关于e的梯度，我们要用到链式法则： \frac{\partial e}{\partial a} = \frac{\partial e}{\partial d} * \frac{\partial d}{\partial a} = c \\ \frac{\partial e}{\partial b} = \frac{\partial e}{\partial d} * \frac{\partial d}{\partial b} = -c因此，节点的梯度可以通过该节点输入时的梯度乘以接下来输出时的梯度计算得到，仔细看几遍上述公式，就可以理解。 然而，实际应用中没这么简单。我们再来看看另外一个例子。考虑一个节点被传入给下一层多个节点的情景，而这种情况在神经网络中也是经常存在的。 这种情况下，除了节点m，其他节点的梯度和上面一样都很容易计算得到，因为m被传输给两个节点了。在此，我将介绍如何计算m的梯度，其余部分自行参照计算。 \frac{\partial o}{\partial n} = p \ \frac{\partial o}{\partial p} = n \\ \frac{\partial n}{\partial m} = 1 \ \frac{\partial p}{\partial m} = 1 \\ \frac{\partial o}{\partial m} = (\frac{\partial o}{\partial n} * \frac{\partial n}{\partial m} ) + (\frac{\partial o}{\partial p} * \frac{\partial p}{\partial m} ) = p + n可以看到，目标梯度是两个不同梯度的总和，我想大家应该看明白了，好好理解一下，接下来继续。先总结下优化一个神经网络的整体过程，每次迭代时需要做的事情如下： 确定一个神经网络结构，包括隐藏层的层数，每一层网络的节点数量，以及激活函数。 随机初始化权重系数。 通过前向传播计算输出。 利用真实标签计算模型的误差。 将误差反向传播到网络，并计算每个节点的误差。 通过更新权重系数以最小化梯度。 至此，我们已经介绍了1-3点，第5点也有所了解。接下来我们来关注4-6点，我们将采用第四小节同样的神经网络结构。 点4-模型的误差e_L^{(i)} = y^{(i)} - a_L^{(i)}上式中，$y^{(i)}$ 是训练样本的实际输出。 点5-误差反向传播到网络首先，第L-1层网络的误差可以通过下式计算得到： e_{L - 1}^{(i)} = (\sum_{k=1}^{N_L} W_{ik}^{L-1} . e_L^{(i)}) * f'(x)^{(i)}其中，i = 0,1,2,…$N_{L-1}$（L-1层网络的节点数量）小节过半，来看下一些概念： 我们知道一个节点的梯度是它下一层网络所有节点梯度的函数。在此，一个节点产生的误差是基于他下一层网络所有节点误差的权重和，而该节点的输出也组成了下一层网络的输出。由于误差又是通过计算节点的梯度得以优化，因此梯度的影响显而易见。 $f’(x)^{(i)}$ 是指变量关于激活函数的导数。注意，x指的是该层所有节点的加权和。 链式规则在这里排上场了，乘上了当前节点的梯度，也就是$f’(x)^{(i)}$与下一层网络节点组合（方程的第一部分）的相乘。 上式需要从第L-1层到第2层连续重复计算，记住，第一层就是训练样本本身。 点6-更新权重系数以最小化梯度用以下规则更新权重系数： W_{ik}^{(l)} = W_{ik}^{(l)} + a^{(i)} . e_{l+1}^{(k)}其中， $l = 1,2,…,(L-1)$，指网络层数的索引（除了最终的输出层）。 $i = 0,1,…N_l$，指第l层网络的节点索引。 $k = 1,2,…N_{l+1}$，指第l+1层网络的节点索引。 $W_{ik}^{(l)}$ 指的是从第l层的第i个节点到第l+1层网络的第k节点的权重。 上式是非常明确的，如有不懂的地方，我建议多看几遍。 结束语 本文的重点是神经网络的基本原理及其工作原理。希望大家现在能理解一个神经网络是如何工作的，从而再也不会把它当作一个黑盒子，一旦你理解了，实际上也很容易，期待着与您进一步交流！ 参考文献：https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost-参数调优]]></title>
    <url>%2F2017%2F05%2FXGBoost-%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98.html</url>
    <content type="text"><![CDATA[简介 如果你的预测建模表现的不尽人意，请使用XGBoost。 XGBoost算法已经成为许多数据科学家的终极武器。这是一个非常复杂的算法，足以处理各种不规则的数据。 XGBoost算法是梯度提升算法（GBM）的改进版，使用XGBoost构建模型非常简单。但是，使用XGBoost改善模型很困难，这个算法包含很多参数。因此要提升模型的效果，调参尤为关键。但是，要说哪组参数要怎么调节、为了得到最佳效果参数应该设为多少，诸如此类的问题，也是很难给出统一答案的。 XGBoost算法的优点 正则化标准的GBM算法并没有正则项，正则项可以有效防止模型过拟合。XGBoost也是以“正则化提升”技术而闻名。 并行计算相比于GBM，XGBoost实现了并行处理，并且速度更快。但众所周知，boosting不是一种串行的结构吗?怎么并行的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。XGBoost支持hadoop实现。 高灵活性算法支持自定义目标函数形式和评价标准。这为模型增加了全新的维度，所以我们的处理不会受到任何限制。 处理缺失值XGBoost有一个内置的常规处理缺失值的方法用户需要提供与其他样本不同的值，并将其作为参数传递。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。 树剪枝对于GBM，当节点在分裂时产生负loss，就会停止分裂。因此，它更像是一种贪婪算法。XGBoost不同，它会先分裂到树的深度为“max_depth”（参数指定），然后回过头来剪枝，如果某个节点之后不再有正值，它会去除这个分裂。另一个优点在于，假设GBM发现分裂产生负loss，如-2，可能接下来继续分列会产生+10的loss，这样的话因为负loss，GBM会提前停止分列，但XGBoost会继续分列，从而会发现-2和+10组合后还有+8的loss，因此两次分列都会保留。 内置交叉验证XGBoost可以在每一轮boosting迭代的时候使用交叉验证。因此，可以方便获得最优boosting迭代的次数。而GBM使用网格搜索，只能检测有限个值。 在已有的模型基础上继续XGBoost可以在上一轮的结果上继续训练，这个特征在某些应用上是一个很大的优势。sklearn中的GBM的实现也有这个功能，两种算法在这一点上是一致的。 XGBoost参数 通用参数booster [默认=gbtree]：$\quad$ 选择每次迭代的基模型：gbtree(树模型)(通常用)、gbliner(线性模型)。silent [默认=0]：$\quad$ 设为1时，静态模式开启，不会有任何输出信息。$\quad$ 所以一般都选择默认0，有助于通过输出信息理解模型。nthread [默认系统最大线程数]：$\quad$ 这个参数用来进行多线程控制，应当输入系统的核数。$\quad$ 如果希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。 booster参数（ 尽管有两种booster可供选择，我这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到）eta [默认=0.3]：$\quad$ 和GBM的学习率(learing rate)类似。$\quad$ 通过减少每一步的权重，可以提高模型的鲁棒性。$\quad$ 典型取值0.01~0.2。min_child_weight [默认=1]：$\quad$ 决定叶子节点样本权重之和的最小值。$\quad$ 和GBM的min_child_leaf类似，但不完全一致，XGBoost是最小样本权重和，GBM是最小样本数。$\quad$ 控制模型过拟合，太大的值会阻碍模型学习局部样本之间的关系。$\quad$ 因此值太大会导致欠拟合，需要通过交叉验证来学习。max_depth [默认=6]：$\quad$ 树的最大深度，和GBM一样。$\quad$ 控制模型过拟合。值过大，会过度学习。$\quad$ 需要通过交叉验证来学习。$\quad$ 一般取值3~10。max_leaf_nodes：$\quad$ 树上最大叶子的数量。$\quad$ 可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成n2个叶子。$\quad$ 如果定义了这个参数，GBM会忽略max_depth参数。gamma[默认=0]：$\quad$ 在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。$\quad$ 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调参的。max_delta_step[默认=0]：$\quad$ 这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。$\quad$ 通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。$\quad$ 这个参数一般用不到，但是你可以挖掘出来它更多的用处。subsample[默认=1]：$\quad$ 和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。$\quad$ 减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。$\quad$ 典型值：0.5-1colsample_bytree[默认=1]：$\quad$ 和GBM里面的max_features参数类似。用来控制每棵树随机采样的列数的占比(每一列是一个特征)。$\quad$ 典型值：0.5-1colsample_bylevel[默认=1]：$\quad$ 用来控制树的每一级的每一次分裂，对列数的采样的占比。$\quad$ 一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。lambda[默认=1]：$\quad$ 权重的L2正则化项。(和Ridge regression类似)。$\quad$ 这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。alpha[默认=1]：$\quad$ 权重的L1正则化项。(和Lasso regression类似)。$\quad$ 可以应用在很高维度的情况下，使得算法的速度更快。scale_pos_weight[默认=1]：$\quad$ 在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。 学习目标参数objective[默认=reg:linear]：$\quad$ binary:logistic，二分类的逻辑回归，返回预测的概率(不是类别)。$\quad$ multi:softmax，使用softmax的多分类器，返回预测的类别(不是概率)。 在这种情况下，你还需要多设一个参数：num_class(类别数目)。$\quad$ multi:softpro，和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。eval_metric[默认值取决于objective参数的取值]：$\quad$ 对于有效数据的度量方法。$\quad$ 对于回归问题，默认值是rmse，对于分类问题，默认值是error。$\quad$ 典型值有： rmse：均方根误差 mae：平均绝对误差 logloss：负对数似然函数值 error：二分类错误率(阈值为0.5) merror：多分类错误率 mlogloss：多分类logloss损失函数 auc：曲线下面积 seed[默认=0]：$\quad$ 随机数的种子。$\quad$ 设置它可以复现随机数据的结果，也可以用于调整参数。 注意：如果你之前用的是Scikit-learn，你可能不太熟悉这些参数。但是有个好消息，Python的XGBoost模块有一个sklearn包-XGBClassifier。这个包中的参数是按sklearn风格命名的，会改变的函数名: eta -&gt; learning_rate lambda -&gt; reg_lambda alpha -&gt; reg_alpha 你肯定在疑惑为啥咱们没有介绍和GBM中的n_estimators类似的参数。XGBClassifier中确实有一个类似的参数，但在标准XGBoost实现调用拟合函数时，把它作为num_boosting_rounds参数传入。 参数调优的一般方法使用和GBM中相似的方法。需要进行如下步骤： 选择较高的学习速率(learning_rate)。一般情况下，学习速率的值为0.1。但是，对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。选择对应于此学习速率的理想决策树数量。XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。 对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth,min_child_weight, gamma,subsample,colsample_bytree)。在确定一棵树的过程中，我们可以选择不同的参数。 xgboost的正则化参数的调优(lambda,alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。 降低学习速率learning_rate，确定理想参数。 第一步：确定学习速率和tree_based参数调优的决策树数量： 其他参数初始化： 1. max_depth = 5：这个参数的取值最好在3-10之间。我选的起始值为5，但是你 也可以选择其它的值。起始值在4-6之间都是不错的选择。 2. min_child_weight = 1：在这里选了一个比较小的值，因为这是一个极不平衡的分类问题。因此，某些叶子节点下的值会比较小。 3. gamma = 0： 起始值也可以选其它比较小的值，在0.1到0.2之间就可以。这个参数后继也是要调整的。 4. subsample, colsample_bytree = 0.8：这个是最常见的初始值了。典型值的范围在0.5-0.9之间。 5. scale_pos_weight = 1：这个值是因为类别十分不平衡。 6. 这里把学习速率就设成默认的0.1。然后用xgboost中的cv函数来确定最佳的决策树数量。 第二步： max_depth 和 min_child_weight 参数调优第三步：gamma参数调优第四步：subsample 和 colsample_bytree调优第五步：正则化参数调优第六步：降低学习速率，使用更多的决策树 参考文献：https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Parameter Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型偏差与方差]]></title>
    <url>%2F2017%2F05%2F%E6%A8%A1%E5%9E%8B%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE.html</url>
    <content type="text"><![CDATA[模型偏差（Bias）和方差（Variance）的权衡对模型的整体性能至关重要。 模型目标函数包含下面两项： Bias（偏差）描述的是预测值的期望与真实值之间的差距，针对的是单个模型，偏差越大，越偏离真实数据，刻画得是模型的拟合能力。 Variance（方差）描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散。度量了训练集的变动所导致的学习性能的变化，即数据扰动造成的影响，方差可以理解为多个模型之间的差异，刻画的是模型的复杂度。 感性理解目标函数的设计来自于统计学习里面的一个重要概念叫做Bias-variance tradeoff。Bias可以理解为假设我们有无限多数据的时候，可以训练出最好的模型所拿到的误差。而Variance是因为我们只有有限数据，其中随机性带来的误差。目标中误差函数鼓励我们的模型尽量去拟合训练数据，这样相对来说最后的模型会有比较少的bias。而正则化项则鼓励更加简单的模型。因为当模型简单之后，有限数据拟合出来结果的随机性比较小，不容易过拟合，使得最后模型的预测更加稳定。 Bias-variance tradeoff 我们为什么要在目标函数中加入偏差和方差？ 优化损失鼓励预测模型 拟合训练数据至少让你靠近的训练数据，希望接近底层分布。 优化正规化鼓励简单模型 简单的模型在预测未来上往往有较小的方差，预测稳定。 如何处理高Bias高Variance （1）增加训练样本可以减小方差，随着样本量的增加，泛化性能会好一些，验证损失会逐渐减小，所以会减小方差。（2）取少量的特征，可以有效的防止过拟合，提高泛化性能，会减小方差。（3）取更多的特征，能从更多的角度学习数据的分布，减小训练损失，会减小偏差。（4）增加多项式特征，增加了模型的复杂度，可以减小偏差。（5）减小正则化参数：就是削弱正则的作用，增加模型复杂度，减小偏差（6） 增大正则化参数：增强正则的作用，对参数进行有效控制，防止过拟合，减小方差。 参考：http://www.52cs.org/?p=429https://www.jianshu.com/p/9d5c5376cacb]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Bias-Variance</tag>
        <tag>Model selection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教学示例文档]]></title>
    <url>%2F2017%2F05%2F%E6%95%99%E5%AD%A6%E7%A4%BA%E4%BE%8B%E6%96%87%E6%A1%A3.html</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>test</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
</search>
